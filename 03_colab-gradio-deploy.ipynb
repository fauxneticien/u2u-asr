{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ed4078-72b7-429c-9474-fd50b2148914",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c54a5-d41d-42ca-9e11-b1d373bbac19",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f9868e-3bac-44dc-af4a-04dd853b177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pympi-ling gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff41980d-2c82-4892-a697-834597c25a70",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c502716-69db-4f49-bbd1-583fb038f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "class mHuBERTFinetuneModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.pad_token_index = self.vocab.index(\"<pad>\")\n",
    "        self.transformer = torchaudio.models.hubert_base()\n",
    "        self.lm_head = torch.nn.Linear(768, len(vocab), bias=False)\n",
    "\n",
    "    @classmethod\n",
    "    def from_finetuned(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=False)\n",
    "        vocab = checkpoint[\"metadata\"][\"vocab\"]\n",
    "\n",
    "        model = mHuBERTFinetuneModel(vocab)\n",
    "        model.load_state_dict(checkpoint[\"weights\"])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def forward(self, audio_padded, audio_lengths=None):\n",
    "        hidden_feats, hidden_lengths = self.transformer(audio_padded, audio_lengths)\n",
    "        logits = self.lm_head(hidden_feats)\n",
    "        logprobs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        return self.decode(logprobs)\n",
    "\n",
    "    def decode(self, logprobs):\n",
    "        indices = torch.argmax(logprobs, dim=-1)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for p in list(indices):\n",
    "            unique_indices = torch.unique_consecutive(p, dim=-1)\n",
    "            prediction = \"\".join([ self.vocab[i] for i in unique_indices if i != self.pad_token_index ])\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217fa86-4b28-4369-91a0-363894914c8d",
   "metadata": {},
   "source": [
    "## Download ASR model\n",
    "\n",
    "This could be using `gdown` to download a link-shared model on Google Drive, `rclone` to download from another shared drive, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4d3a8c-8e1f-4496-a07a-c52e656da480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random init model to mock checkpoint downloaded from elsewhere\n",
    "!mkdir -p /content\n",
    "\n",
    "import string\n",
    "\n",
    "mock_vocab = ['<pad>'] + list(string.ascii_lowercase)\n",
    "mock_model = mHuBERTFinetuneModel(vocab=mock_vocab)\n",
    "torch.save(\n",
    "    { \"metadata\" : { \"vocab\" : mock_vocab }, \"weights\" : mock_model.state_dict() },\n",
    "    \"/content/mock_checkpoint.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc21cd-1d50-41a1-8f3f-ae94ae386d7f",
   "metadata": {},
   "source": [
    "## Load VAD and ASR Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e9cbc4-1559-4cc5-a1eb-d3b8af3b57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mHuBERTFinetuneModel.from_finetuned(\"/content/mock_checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a5ad68f-a970-46d5-a338-0f3ab307adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "vad_model, vad_utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False, trust_repo=True)\n",
    "(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = vad_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c56042b-d822-4643-a8fb-107506649609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pympi import Elan\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def print_silero_progress(silero_pc):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(\"[%-100s] %d%%\" % ('='*round(silero_pc), silero_pc))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if silero_pc == 100.0:\n",
    "        print(\"\\n\", end=\"\")\n",
    "\n",
    "def transcribe(audio_filepath, one_tier_per_channel):\n",
    "\n",
    "  audio_filepath = Path(audio_filepath)\n",
    "\n",
    "  waveform, sr = torchaudio.load(audio_filepath)\n",
    "\n",
    "  # If needed, resample audio to 16 kHz for Silero-VAD and HuBERT\n",
    "  if sr != 16_000:\n",
    "    waveform = torchaudio.functional.resample(waveform, sr, 16_000)\n",
    "\n",
    "  # If not mono and not set to one tier per channel then convert to mono\n",
    "  if waveform.size(0) > 1 and not one_tier_per_channel:\n",
    "    waveform = waveform.mean(axis=0, keepdim=True)\n",
    "\n",
    "  eaf_data = Elan.Eaf()\n",
    "  eaf_data.add_linked_file(audio_filepath.name)\n",
    "  # Remove 'default' tier from newly created eaf object\n",
    "  eaf_data.remove_tier('default')\n",
    "\n",
    "  for channel in range(waveform.size(0)):\n",
    "\n",
    "    eaf_data.add_tier(f\"Channel {channel}\")\n",
    "\n",
    "    print(f\"Detecting time regions with speech on channel {channel}\")\n",
    "\n",
    "    channel_waveform = waveform[channel, :].unsqueeze(0)\n",
    "\n",
    "    speech_timestamps = get_speech_timestamps(channel_waveform, vad_model, threshold=0.75, progress_tracking_callback=print_silero_progress)\n",
    "\n",
    "    print(f\"Transcribing speech in detected regions on channel {channel}\")\n",
    "\n",
    "    for segment_bounds in tqdm(speech_timestamps):\n",
    "      segment_samples=channel_waveform[:, segment_bounds['start']:segment_bounds['end']].cuda()\n",
    "\n",
    "      with torch.inference_mode():\n",
    "        audio_normed = torch.nn.functional.layer_norm(segment_samples, segment_samples.shape)\n",
    "        text = model(audio_normed)[0].strip()\n",
    "\n",
    "      start_ms, end_ms = [segment_bounds['start']/16, segment_bounds['end']/16]\n",
    "      eaf_data.add_annotation(f\"Channel {channel}\", start=round(start_ms), end=round(end_ms), value=text)\n",
    "\n",
    "  eaf_file = audio_filepath.with_suffix(\".eaf\")\n",
    "  eaf_data.to_file(eaf_file)\n",
    "  \n",
    "  return str(eaf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80253e9b-4057-42c3-87ff-d15e37599b9a",
   "metadata": {},
   "source": [
    "## Create Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc062b-cd02-4061-86f2-f8cce7d98c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "asr = gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=[ \n",
    "        gr.Audio(type=\"filepath\"),\n",
    "        gr.Checkbox(label=\"Yes\", info=\"Create one tier per audio channel?\", value=True)\n",
    "    ],\n",
    "    outputs=gr.File(label=\"ELAN eaf file\"),\n",
    "    title=\"Transcribe Audio to ELAN eaf file\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9d874-295e-430e-88aa-c6ac421c92c5",
   "metadata": {},
   "source": [
    "# Run app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038cfafd-b692-4a0d-917a-6666723a238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
